import openai
import os
import tiktoken # For consistent token counting with OpenAI API
import pandas as pd
from tqdm import tqdm



class GPTScoreEvaluatorOpenAI:
    def __init__(self, model_name: str = "gpt-3.5-turbo"):
        # Ensure your OPENAI_API_KEY environment variable is set.
        # Example: export OPENAI_API_KEY="sk-YOUR_KEY_HERE"
        if os.getenv("OPENAI_API_KEY") is None:
            raise ValueError("OPENAI_API_KEY environment variable not set. Please set it.")
        
        self.model_name = model_name
        self.tokenizer = tiktoken.encoding_for_model(self.model_name)
        print(f"GPTScoreEvaluator initialized with OpenAI model: {self.model_name}")

    def score(self, generated_text: str, task_description: str, aspect: str, context: str = None, demonstrations: list[str] = None) -> float:
        prompt_parts = []
        if task_description:
            prompt_parts.append(task_description)
        if context:
            prompt_parts.append(f"Context: {context}")
        if demonstrations:
            for i, demo in enumerate(demonstrations):
                prompt_parts.append(f"Example {i+1}:\n{demo}")
        
        prompt_parts.append(f"Aspecto de Avaliação: {aspect}")
        prompt_parts.append(f"Texto a ser avaliado: ")

        full_prompt_prefix = "\n\n".join(prompt_parts)
        
        # Estimate number of tokens for the generated text
        generated_tokens_count = len(self.tokenizer.encode(generated_text))

        try:
            # Call OpenAI Chat Completions API
            completion = openai.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": full_prompt_prefix}],
                max_tokens=generated_tokens_count + 5, # Request slightly more tokens to ensure full coverage
                logprobs=True,
                top_logprobs=1, # We only need the logprob for the most likely token
                temperature=0.0 # Aim for deterministic generation to make logprobs more meaningful
            )

            if not completion.choices or not completion.choices[0].logprobs or not completion.choices[0].logprobs.content:
                print("Could not retrieve logprobs from OpenAI API response.")
                return 0.0

            total_log_prob = 0.0
            
            # Sum logprobs for the tokens generated by the OpenAI model
            # This approximates GPTSCORE by evaluating the probability of the model
            # *generating* the provided text given the prompt.
            for token_data in completion.choices[0].logprobs.content:
                total_log_prob += token_data.logprob

            return total_log_prob

        except openai.APIError as e:
            print(f"OpenAI API Error: {e}")
            return 0.0
        except Exception as e:
            print(f"An unexpected error occurred: {e}")
            return 0.0


## Example Usage

if __name__ == "__main__":
    # Choose your OpenAI model. For GPTSCORE, models with good text generation
    # and logprob support are ideal. "gpt-3.5-turbo" or "gpt-4" are good choices.
    evaluator = GPTScoreEvaluatorOpenAI(model_name="gpt-3.5-turbo")

    generated_text_good = "O gato pulou agilmente sobre o muro alto, perseguindo um pequeno rato."
    generated_text_bad = "Gato pulo muro rato pequeno."
    
    task_desc = "Responda à pergunta com uma frase completa e gramaticalmente correta."
    aspect_eval = "A fluência do texto é boa?"
    context_info = "Pergunta: O que o gato fez?"
    
    demonstrations_list = [
        "Exemplo bom: O cão correu alegremente pelo campo verde. (Fluência: 5/5)",
        "Exemplo ruim: Cão correu campo verde. (Fluência: 1/5)"
    ]

    print(f"Evaluating with OpenAI model: {evaluator.model_name}")

    score_good = evaluator.score(
        generated_text=generated_text_good, 
        task_description=task_desc, 
        aspect=aspect_eval, 
        context=context_info,
        demonstrations=demonstrations_list
    )
    print(f"GPTSCORE for 'good' text: {score_good:.4f}")

    score_bad =     )
    print(f"GPTSCORE for 'bad' text: {score_bad:.4f}")

    print("\nNote: A higher (less negative) GPTSCORE indicates higher quality as perceived by the OpenAI model.")

    resultados = []
    geval_evaluator = GEvalEvaluator()
    models = [
        "gpt-3.5-turbo",
        "llama-3.1-8b-instant",
        "llama-3.3-70b-versatile",
        "llama3-8b-8192",
        "llama3-70b-8192",
        "whisper-large-v3",
        "whisper-large-v3-turbo",
    ]

    for model in models:
        path = f"./data/{model}_answers.csv"
        df = pd.read_csv(path)

        df["Resposta"] = df["Resposta"].fillna("resposta ausente")

        match = re.search(r"\./data/([a-zA-Z0-9\-_.]+)\.csv", path)
        nome_base = None
        if match:
            nome_base = match.group(1)
            print(nome_base)

        if nome_base:
            for _, row in tqdm(df.iterrows(), total=len(df)):
                if (
                    pd.isnull(row["Pergunta"])
                    or pd.isnull(row["Resposta"])
                    or pd.isnull(row["Resposta_Gerada"])
                ):
                    continue
                

                evaluator.score(
                    generated_text=row["Resposta_Gerada"], 
                    task_description=task_desc, 
                    aspect=aspect_eval, 
                    context=context_info,
                    demonstrations=demonstrations_list
                )

                if evaluation_case is None:
                    continue

                resultados.append(
                    {
                        "Pergunta": row["Pergunta"],
                        "Resposta_Esperada": row["Resposta"],
                        "Resposta_Gerada": row["Resposta_Gerada"],
                        "Score": evaluation_case.outputs[metric_name].score,
                        "Passou": evaluation_case.outputs[metric_name].is_successful(),
                        "Justificativa": evaluation_case.outputs[metric_name].reason,
                    }
                )
            pd.DataFrame(resultados).to_csv(f"./resultados/gptscore/{nome_base}.csv")
